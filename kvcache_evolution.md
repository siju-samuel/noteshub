# ðŸ§© **1. What is KV Cache?**

Transformers store:
- **K = Key vectors**
- **V = Value vectors**

for each token during decoding.

This avoids recomputing attention on previous tokens.

---

# âœ… **2. Why KV Cache is Essential**

Without KV cache:  
â†’ Each new token requires O(NÂ²) attention.

With KV cache:  
â†’ Each new token requires O(N) compute.

---

```mermaid
flowchart TD
    A["Token 1 â†’ Compute K1,V1"] --> B["Store in KV Cache"]
    C["Token 2 â†’ Compute K2,V2"] --> B
    D["Token 3"] --> E["Read existing KV (1..2)"]
    E --> F["Compute Attention"]
```

---

# âœ… **3. KV Cache Memory Problem**

KV cache grows linearly with sequence length Ã— layers Ã— batch size.

Long prompts (100kâ€“1M tokens) â†’ KV cache dominates GPU memory.

---

# âœ… **4. Evolution of KV Cache Management**

```mermaid
flowchart TD
    A["Naive KV Cache"] --> B["Static Preallocated KV"]
    B --> C["Paged KV Cache"]
    C --> D["PagedAttention Kernel"]
    D --> E["LMCache (Multi-level KV)"]
    E --> F["Parameter & Activation Disaggregation"]
    E --> G["PD: Prefill/Decode Disaggregation"]
```

---

# âœ… **5. Static KV Cache (Early Implementations)**

- Big contiguous buffer
- Fragmentation
- Hard to support multi-user batches
- OOM common

---

# âœ… **6. Paged KV Cache (vLLM Breakthrough)**

Break KV cache into **fixed-size blocks**, just like OS memory pages.

```mermaid
flowchart TD
    A["Logical KV Cache"] --> B["Page Table"]
    B --> C["Block 0"]
    B --> D["Block 1"]
    B --> E["Block 2"]
    B --> F["Block 3"]
```

âœ… Avoids fragmentation  
âœ… Allows reuse of freed KV pages  
âœ… Enables continuous batching  

---

# âœ… **7. PagedAttention â€” Page-aware GPU Kernel**

PagedAttention uses:
- Block pointers
- Coalesced block loads
- Continuous block reuse

```mermaid
flowchart LR
    A["Token Embedding"] --> B["KV Generation"]
    B --> C["Paged KV Blocks"]
    C --> D["Attention Kernel (PagedAccess)"]
    D --> E["Next Token"]
```

âœ… High throughput  
âœ… Enables 4â€“10Ã— more concurrent users  

---

# âœ… **8. LMCache â€” Multiâ€‘Level KV Cache (GPU â†’ CPU â†’ NVMe)**

When prompts exceed GPU memory, we need a **tiered KV storage hierarchy**:

```mermaid
flowchart TD
    A["GPU KV Cache - L1"] --> B["CPU KV Cache - L2"]
    B --> C["NVMe KV Cache - L3"]
```

L1 = Fastest, smallest  
L2 = Medium capacity  
L3 = Very large but slow  

LMCache implements:
- Prefetching  
- Eviction  
- Async background transfers  

âœ… Enables millionâ€‘token context inference  

---

# âœ… **9. Parameter & Activation Disaggregation**

Goal: Store as little as possible on GPU.

### âœ… Parameter Disaggregation
Weights live in:
- Host RAM
- NVMe SSD
- Remote parameter servers

### âœ… Activation / KV Disaggregation
KV lives across:
- GPU
- CPU
- NVMe
- Remote nodes

```mermaid
flowchart TD
    A["GPU Compute"] --> B["Is Layer Weight Local?"]
    B -->|No| C["Fetch Weight from CPU/NVMe"]
    B -->|Yes| D["Use Local Weight"]

    D --> E["Need KV Block?"]
    E -->|GPU| F["Use GPU KV"]
    E -->|CPU| G["Fetch from CPU"]
    E -->|NVMe| H["Fetch from NVMe"]
    E -->|Remote| I["Fetch from Remote Node"]

    F --> J["Attention Compute"]
    G --> J
    H --> J
    I --> J
```

âœ… Enables **trillionâ€‘parameter scale**  
âœ… Works with hybrid memory architectures  

---

# âœ… **10. Prefill/Decode (PD) Disaggregation (vLLM, SGLang)**

PD splits inference into two separate distributed systems:

```mermaid
flowchart LR
    A["Prefill Workers"] --> B["Prefill Outputs (KV+Hidden)"]
    B --> C["Decode Workers"]
```

### **Prefill Workers**
- Handle heavy GEMM throughput
- Batch many requests

### **Decode Workers**
- Handle autoregressive loop
- Use cached K/V from prefill

Benefits:
âœ… Multi-host parallelism  
âœ… Better GPU utilization  
âœ… Improved tail latency  
âœ… Great for longâ€‘context workloads  

---

# âœ… **Final Summary**

| Technique | Purpose | Key Benefit |
|----------|----------|--------------|
| Static KV | Preallocated KV buffers | Simple but wasteful |
| Paged KV | KV blocks + page tables | Efficient multi-tenant inference |
| PagedAttention | Page-aware kernels | High throughput |
| LMCache | Multi-level KV (GPU/CPU/NVMe) | Long-context expansion |
| Parameter & Activation Disaggregation | Split model/KV across memory tiers | Run ultra-large models |
| Prefill/Decode Disaggregation | Split inference workflow | Higher throughput & concurrency |

---
